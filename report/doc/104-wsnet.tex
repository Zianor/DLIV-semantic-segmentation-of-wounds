\section{WSNet}

The following section describes the assessment and evaluation of WSNet. This starts with implementing the tested models and checking for deviations between the described methods and available code. Several experiments are performed to assess the algorithm in more detail and check its robustness.

\subsection{Code availability and reproduction of the results}

Although the code for WSNet \cite{Oota_2023_WACV} is stated to be publicly available, a closer inspection of the linked GitHub repository shows that this is only partially the case. A lack of documentation makes using the code hard, especially since it seems to contain multiple errors, making it only suitable as a base for new code.

In this project's scope, the code was used to create runnable models again. Unfortunately, the classes of the wounds are not available, making it impossible to perform pre-training as described in the original paper \cite{Oota_2023_WACV}. Eight models are available: A local model and a combined global-local model for each segmentation model: U-Net, PSPNet, FPN, and LinkNet. The Python library used for the segmentation models is \texttt{segmentation\_models} \cite{SegmentationModels}. The implementation process showed some differences from the described model architecture. In particular, it was claimed that the wound images were split up in parts of 48\,px times 48\,px. However, three of the four models, all besides PSPNet, only allow input sizes divisible by 32, and the code in GitHub showed a size of 64\,px was used. Another difference between the available code and the paper is that it is claimed that augmentation is not performed on the test images, which is not the case.

The paper or code does not give information about the training, validation and test set size. However, the code reveals that the train and validation sets were just the first x\,\% of the dataset, and no randomisation was used to separate the test set as it is usually done. In this project's scope, a split of 70\,\% training, 15\,\% validation and 15\,\% test data is used.

Because the data training for the wound-specific pre-training is not available, the results can only be compared for ImageNet pre-training. An important aspect is that MobileNet has no pre-trained weights for images with the size of the patches, and the default size of 224x224\,px is used instead, which might impact results negatively.

The loss and activation functions used were a Dice-Loss function and a sigmoid activation function.

\subsection{Comparison of the achieved performance}

The results achieved with the 70:15:15 split of the data and MobileNet as backbone are shown in Table \ref{fig:results-own}. Generally, the results are comparable to the results reported by \citeauthor{Oota_2023_WACV} (shown in Table \ref{table:results-wsnet}), although the performance achieved in this project is slightly lower. Some deviations are more significant. For example, the Global-Local model with U-Net achieved an IoU score of 0.495 while \citeauthor{Oota_2023_WACV} reported a score of 0.620 (Dice score 0.658 vs. 0.763). Other scores are closer, e.g., the Global-Local model with LinkNet achieved IoE scores of 0.618 vs. 0.621, respectively and a Dices score of 0.763. Such deviations could be based on a different training size.

\begin{table}[htb!]
	\centering
	\begin{tabular}{l||c | c | c | c | c | c | c | c|}
	& \multicolumn{2}{|c|}{U-Net} & \multicolumn{2}{|c|}{LinkNet} & \multicolumn{2}{|c|}{PSPNet} & \multicolumn{2}{|c|}{FPN} \\
	\hline
	& IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\textbf{Local model} & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\	
	\textbf{Global model} & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	\textbf{Global-Local model} & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four models with each Global-Local, Global and Local architecture. The backbone used is MobileNet.}
	\label{fig:results-own}
\end{table}

A more important finding is that in the re-implemented models, the Global-Local model does not necessarily lead to an improved performance compared to the Global model. So, the additional use of local information in a specific region size does not necessarily improve the overall results. However, the results reported here are only for MobileNet as the backbone, where no weights specific to the patch size could be loaded. But even with a slightly improved performance, it is still arguable that using a Global-Local model is not the best choice since it includes some drawbacks: First, it is computationally significantly more complex and second, it implies that the chosen patch size has a special meaning.

\subsection{Experiments with the activation function}

The original papers describing the four used model architectures mainly mention ReLU as the activation function. However, \citeauthor{Oota_2023_WACV} use Sigmoid in all their experiments. Because no clear findings were found on whether one of the activation functions is more appropriate, an experiment was performed comparing both for all model architectures.

\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Activation & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{2}{*}{\textbf{Local model}} & Sigmoid & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& ReLU & 0.398 & 0.565 & 0.396 & 0.561 & 0.372 & 0.536 & 0.380 & 0.546 \\
	\hline
	\multirow{2}{*}{\textbf{Global model}} & Sigmoid & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& ReLU & 0.513 & 0.676 & 0.509 & 0.672 & 0.463 & 0.631 & 0.505 & 0.669 \\
	\hline
	\multirow{2}{*}{\textbf{Global-Local model}} & Sigmoid & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& ReLU & 0.498 & 0.662 & 0.588 & 0.738 & 0.569 & 0.724 & 0.610 & 0.756 \\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for the Sigmoid and ReLU activation function.}
	\label{table:sigmoid-relu-comparison}
\end{table}

The results, reported in Table \ref{table:sigmoid-relu-comparison}, show no clear trend of one of the functions performing higher on all model architectures. Therefore, the chosen activation function is excluded as a factor for further experiments.

\subsection{Combination of different architectures}

Since all four model architectures localise signals differently by design, the resulting local and global feature information also differ. Following \citeauthor{Oota_2023_WACV} and assuming the inclusion of local models increases the performance, combining different architectures in the Global-Local models might be interesting. This was assessed by training and evaluating all possible combinations of the models in the Global-Local architectures. The results are reported in Table \ref{table:global-local-mixed}.

\begin{table}[htb!]
	\centering
	\begin{tabular}{c|c| c| c}
		Global Model & Local Model & IoU & Dice \\ \hline\hline
		\multirow{4}{*}{U-Net} & U-Net & 0.495 & 0.658\\
		 & LinkNet  & 0.602 & 0.749 \\
		 & PSPNet & 0.607 & 0.753 \\
		 & FPN & 0.613 & 0.757 \\\hline
		 \multirow{4}{*}{LinkNet} & LinkNet & 0.618 & 0.763 \\
		 & U-Net  & 0.633 & 0.774 \\
		 & PSPNet & 0.612 & 0.757 \\
		 & FPN & 0.613 & 0.758 \\\hline
		 \multirow{4}{*}{PSPNet} & PSPNet & 0.476 &  0.642\\
		 & U-Net  & 0.554 & 0.711 \\
		 & LinkNet & 0.576 & 0.729 \\
		 & FPN & 0.580 & 0.732 \\\hline
		 \multirow{4}{*}{FPN} & FPN & 0.612 & 0.758 \\
		 & U-Net  & 0.605 & 0.752 \\
		 & LinkNet & 0.585 & 0.735 \\
		 & PSPNet & 0.627 & 0.769 \\\hline
	\end{tabular}
	\caption{The performance of Global-Local models with all possible architecture combinations. MobileNet is used as backbone and Sigmoid as activation function}
	\label{table:global-local-mixed}	
\end{table}

Generally, combining two different architectures leads to similar or better performance than using the same architecture in local and global models. An exception is FPN as the global model with LinkNet as the local model. Two combinations stand out in their performance: U-Net as the global and local model and PSPNet as the global and local model. Both combinations have a lower performance, with an IoU score below 0.5. Linknet and FPN as global models perform slightly better than the other two. That is consistent with the performance of the global-only models reported before. Those architectures seem to extract slightly better features for wound segmentation.

\subsection{Assessing the Robustness}

As already discussed, one problem of wound segmentation is the diversity of available wound images. Therefore, segmentation should be robust and work for various image types. To assess the robustness of the models, two experiments were performed: Testing the performance on augmented images and testing the performances on another data set without re-training the models.

\subsubsection{Robustness against image augmentations}

As already discussed, augmentations are commonly performed to improve models' robustness. Analogously, augmentations can be used to assess the robustness of the resulting model during testing. In the clinical application of wound segmentation, this includes various lighting conditions, varying image quality and different image sizes. Augmentations are performed on the test set to assess the models' robustness regarding these conditions. Because a trained model ideally should be able to deal with data acquired from different settings that were not included in the training, no specific training for the tested augmentations was done.

Tensorflow image functions were used to augment the training set. This makes it possible to use the already created test dataset containing tensors and transform it in a specific way. The implemented and evaluated augmentations are the following:

\begin{description}[leftmargin=10px]
	\item[Embed] The test images were resized to be smaller than the original. The resized image was positioned in the centre, and the remaining space was filled with black. This tests the model against changes in the size of the wound and background changes, which is a very likely case for a real-world application. Additionally, the models can only deal with square images and rescaling and padding non-square images is a potential solution for pictures in other formats.
	\item[Brightness] Changes in brightness are also likely to happen. A brightness change with a delta of 0.1 is used in the experiment but can be adapted to test multiple scenarios.
	\item[Saturation] This augmentation adapts the image's saturation, which can happen due to various changes in the camera setting. The saturation is changed by a factor of 2 in this experiment.
	\item[Contrast] A change in contrast might happen due to different camera and lighting settings. In this augmentation, the saturation is increased by a factor of 2 in this experiment. 
\end{description}

In this project's scope, only one augmentation is done at a time, although the experiment can be extended to include more augmentations and combinations of multiple simultaneous augmentations. The results are reported in Table \ref{table:augmentation-comparison}. Generally, changes in brightness influence the performance the least. In terms of the performance of local, global and global-local architectures, it is clear that local models consistently perform worse and that their performance is not impacted less than one of the other architectures. The performance of the Global-Local and Global models are affected similarly, except for the FPN Global-Local model, which is impacted more than the Global model. These findings support that Global-Local models are not superior to Global-only models.

\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Augmentation & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{5}{*}{\textbf{Local model}} & - & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& Embed & 0.365 & 0.528 & 0.378 & 0.545 & 0.373 & 0.534 & 0.383 & 0.550\\
	& Brightness & 0.341 & 0.503 & 0.391 & 0.557 & 0.348 & 0.510 & 0.417 & 0.583\\
	& Contrast & 0.297 & 0.454 & 0.286 & 0.442 & 0.270 & 0.422 & 0.206 & 0.338\\
	& Saturation & 0.310 & 0.470 & 0.284 & 0.396 & 0.245 & 0.390 & 0.211 & 0.346\\
	\hline
	\multirow{5}{*}{\textbf{Global model}} & - & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& Embed & 0.400 & 0.566 & 0.438 & 0.607 & 0.333 & 0.497 & 0.454 & 0.622\\
	& Brightness & 0.500 & 0.663 & 0.629 & 0.770 & 0.452 & 0.620 & 0.625 & 0.767\\
	& Contrast & 0.404 & 0.573 & 0.539 & 0.699 & 0.334 & 0.499 & 0.562 & 0.718\\
	& Saturation & 0.420 & 0.586 & 0.475 & 0.641 & 0.346 & 0.511 & 0.441 & 0.609\\
	\hline
	\multirow{5}{*}{\textbf{Global-Local model}} & - & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& Embed & 0.372 & 0.541 & 0.451 & 0.619 & 0.393 & 0.559 & 0.375 & 0.542\\
	& Brightness & 0.495 & 0.659 & 0.613 & 0.759 & 0.465 & 0.632 & 0.604 & 0.751\\
	& Contrast & 0.408 & 0.577& 0.545 & 0.704 & 0.414 & 0.583 & 0.503 & 0.666\\
	& Saturation & 0.402 & 0.570 & 0.491 & 0.657 & 0.387 & 0.555 & 0.490 & 0.654\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for different augmentations on the test images.}
	\label{table:augmentation-comparison}
\end{table}

\subsubsection{Performance on an unseen data set}

A second experiment evaluates the performance on an additional, unseen data set. For this purpose, the data set of the Diabetes Foot Ulcer Segmentation Challenge 2021 \cite{Wang2020} is used. More details are in section \ref{sec:data-sets}. The non-augmented images are loaded, resized to 192x192\,px, and then tested with the already trained models. The performance metrics are displayed in table \ref{table:dataset-comparison}, in comparison with the performance on test data of the original WSNet data set.


\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Data set & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{2}{*}{\textbf{Local model}} & WSNet & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& DFUC & 0.252 & 0.399 & 0.199 & 0.330 & 0.227 & 0.366 & 0.230 & 0.369\\
	\hline
	\multirow{2}{*}{\textbf{Global model}} & WSNet & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& DFUC & 0.205 & 0.336 & 0.263 & 0.411 & 0.150 & 0.258 & 0.220 & 0.355\\
	\hline
	\multirow{2}{*}{\textbf{Global-Local model}} & WSNet & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& DFUC & 0.192 & 0.317 & 0.168 & 0.285 & 0.172 & 0.290 & 0.200 & 0.328\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for the WSNet and the Diabetes Foot Ulcer Segmentation Challenge (DFUC) 2021 data.}
	\label{table:dataset-comparison}
\end{table}

The models' performance drops rapidly for the unknown data set. This shows that the models are not generalising yet for other datasets, and results achieved with the WSNet data set cannot be directly transferred to different data sets. It shows how crucial it is to include diverse data collection in training and that focusing on a specific kind of wound changes the model's learned features.

