\section{WSNet}

The following section describes the assessment and evaluation of WSNet. This starts with an implementation of the tested models and with checking for deviations between described methods and available code. Following that, several experiments are performed to assess the algorithm in more detail and to check its robustness.

\subsection{Code availability and reproduction of the results}

Although the code for WSNet \cite{Oota_2023_WACV} is stated to be publicly available, a closer inspection of the linked GitHub repository shows that this is only partially the case. A lack of documentation makes using the code hard, especially since it seems to contain multiple errors, making it only suitable as a base for new code.

In this project's scope, the code was used to create runnable models again. Unfortunately, the classes of the wounds are not available, making it impossible to perform pre-training as described in the original paper \cite{Oota_2023_WACV}. In total, there are eight models available: A local model and a combined global-local model for each of the segmentation models: U-Net, PSPNet, FPN, and LinkNet. The Python library used for the segmentation models is \texttt{segmentation\_models} \cite{SegmentationModels}. The implementation process showed some differences from the described model architecture. In particular, it was claimed that the wound images were split up in parts of 48\,px times 48\,px. However, three of the four models, all besides PSPNet, only allow input sizes divisible by 32, and the code in GitHub showed a size of 64\,px was used. Another difference between the available code and the paper is that it is claimed that augmentation is not performed on the test images, which is not the case.

Information about the training, validation and test set size is not given in the paper or code. However, the code reveals that the train and validation sets were just the first x\,\% of the dataset, and no randomisation was used to separate the test set as it is usually done. In this project's scope, a split of 70\,\% training, 15\,\% validation and 15\,\% test data is used.

Because the data training for the wound-specific pre-training is not available, the results can only be compared for imagenet pre-training. An important aspect is, that MobileNet has no pretrained weights for images with the size of the patches and the default size of 224x224\,px is used instead, which might impact results negatively.

The used loss and activation functions were a Dice-Loss function and sigmoid as activation function.

\subsection{Comparison of the achieved performance}

The results achieved with the 70:15:15 split of the data and MobileNet as backbone are shown in Table \ref{fig:results-own}. Generally, the results are comparable to the results reported by \citeauthor{Oota_2023_WACV} (shown in Table \ref{table:results-wsnet}), although the performance achieved in this project is slightly lower. Some deviations are bigger, for example the Global-Local model with U-Net achieved an IoU score of 0.495 while \citeauthor{Oota_2023_WACV} reported a score of 0.620 (Dice score 0.658 vs. 0.763). Other scores are closer, e.g., the Global-Local model with LinkNet that achieved IoE scores of 0.618 vs. 0.621 respectively and a Dices score of 0.763. Such deviations could be based in a different training size.

\begin{table}[htb!]
	\centering
	\begin{tabular}{l||c | c | c | c | c | c | c | c|}
	& \multicolumn{2}{|c|}{U-Net} & \multicolumn{2}{|c|}{LinkNet} & \multicolumn{2}{|c|}{PSPNet} & \multicolumn{2}{|c|}{FPN} \\
	\hline
	& IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\textbf{Local model} & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\	
	\textbf{Global model} & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	\textbf{Global-Local model} & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture. The backbone used is mobilenet.}
	\label{fig:results-own}
\end{table}

A more important finding is that in the re-implemented models the Global-Local model does not neccessarily lead to an improved performance compared to the Global model. So the additional use of local information in a specific region size dose not necessarily improve the overall results. However, the results reported here are only for MobileNet as backbone where no weights specific to the patch size could be loaded. But even with a slight improved performance it is still arguable that the use of a Global-Local model is not the best choice since it includes some drawbacks: First, it is computationally significantly more complex and second implies that the chosen patch size has a special meaning.

\subsection{Experiments with the activation function}

The original papers describing the four used model architectures mostly mention ReLU as activation function. However, \citeauthor{Oota_2023_WACV} use Sigmoid in all their experiments. Because no clear findings were find on whether one of the activation functions is more appropriate, an experiment comparing both for all model architectures was performed.

\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Activation & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{2}{*}{\textbf{Local model}} & Sigmoid & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& ReLU & 0.398 & 0.565 & 0.396 & 0.561 & 0.372 & 0.536 & 0.380 & 0.546 \\
	\hline
	\multirow{2}{*}{\textbf{Global model}} & Sigmoid & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& ReLU & 0.513 & 0.676 & 0.509 & 0.672 & 0.463 & 0.631 & 0.505 & 0.669 \\
	\hline
	\multirow{2}{*}{\textbf{Global-Local model}} & Sigmoid & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& ReLU & 0.498 & 0.662 & 0.588 & 0.738 & 0.569 & 0.724 & 0.610 & 0.756 \\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for the Sigmoid and ReLU activation function.}
	\label{table:sigmoid-relu-comparison}
\end{table}

The results, reported in Table \ref{table:sigmoid-relu-comparison}, show that there is no clear trend of one of the functions performaning higher on all model architectures. Therefore, the chosen activation function is excluded as factor for further experiments.

\subsection{Combination of different architectures}

Since all four model architectures localize signals differently by design, the resulting local and global feature information also differ from each other. Following \citeauthor{Oota_2023_WACV} and assuming the inclusion of local models increases the performance, it might be interesting to combine different architectures in the Global-Local models. This was assesed by training and evaluating all possible combinations of the models in the Global-Local architectures. The results are reported in Table \ref{table:global-local-mixed}.

\begin{table}[htb!]
	\centering
	\begin{tabular}{c|c| c| c}
		Global Model & Local Model & IoU & Dice \\ \hline\hline
		\multirow{4}{*}{U-Net} & U-Net & 0.495 & 0.658\\
		 & LinkNet  & 0.602 & 0.749 \\
		 & PSPNet & 0.607 & 0.753 \\
		 & FPN & 0.613 & 0.757 \\\hline
		 \multirow{4}{*}{LinkNet} & LinkNet & 0.618 & 0.763 \\
		 & U-Net  & 0.633 & 0.774 \\
		 & PSPNet & 0.612 & 0.757 \\
		 & FPN & 0.613 & 0.758 \\\hline
		 \multirow{4}{*}{PSPNet} & PSPNet & 0.476 &  0.642\\
		 & U-Net  & 0.554 & 0.711 \\
		 & LinkNet & 0.576 & 0.729 \\
		 & FPN & 0.580 & 0.732 \\\hline
		 \multirow{4}{*}{FPN} & FPN & 0.612 & 0.758 \\
		 & U-Net  & 0.605 & 0.752 \\
		 & LinkNet & 0.585 & 0.735 \\
		 & PSPNet & 0.627 & 0.769 \\\hline
	\end{tabular}
	\caption{The performance of Global-Local models with all possible architecture combinations. MobileNet is used as backbone and sigmoid as activation function}
	\label{table:global-local-mixed}	
\end{table}

TODO: interpret results

\subsection{Assessing the Robustness}

As already discussed, one problem of wound segmentation is the diversity of available wound images. A segmentation should therefore be robust and work for a variety of image types. To asses the robustness of the models, two experiments were performed: Testing the performance on augmented images and testing the performances on another data set without re-training the models.

\subsubsection{Robustness against image augmentations}

Augmentations are, as already discussed, commonly performed to improve the robustness of models. Analogously, augmentations can be used to access the robustness of the resulting model during testing. For the clinical application ouf wound segmentation, this includes various lightning conditions, varying image quality and various image sizes. To assess the models' robustness regarding this conditions, augmentations are performed on the test set. Because a trained model ideally should be able to deal with data acquired from different settings that was not included in training, no specific training for the tested augmentations was done.

To augment the training set, tensorflow image functions were used. This makes it possible to use the already created test dataset containing tensors and transform it in a specific way. The implemented and evaluated augmentations are the following:

\begin{description}[leftmargin=10px]
	\item[Embed] The test images were resized to be smaller than the original. The resized image was positioned in the center and remaining space was filled with black colour. This tests the model against changes in size of the wound and background changes, which is a very likely case for a real-world application. Additionally, the models can only deal with square images and rescaling and padding non-square images is a likely solution for images in other formats.
	\item[Brightness] Changes in brightness are likely to happen as well. A brightness change with a delta of 0.1 is used in the experiment but can be adapted to test multiple scenarios.
	\item[Saturation] This augmentation adapts the saturation of the image, which can happen due to various changes in the camera setting. The saturation is changed by a factor of 2 in this experiment.
	\item[Contrast] A change in contrast might happen due to different camera and lightning settings. In this augmentation, the saturation is increased by a factor of 2 in this experiment. 
\end{description}

In the scope of this project, only one augmentation is done at a time, although the experiment can be extended to include more augmentations and combinations of multiple simultaneous augmentations. The results are reported in Table \ref{table:augmentation-comparison}. Generally, changes in brightness influence the performance the least. 

\begin{itemize}
	\item local models generally worst
	\item embed did not generally impacts global-local model the most, but for fpn, it is significantly worse than when just using a global model
\end{itemize}
TODO

\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Augmentation & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{5}{*}{\textbf{Local model}} & - & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& Embed & 0.365 & 0.528 & 0.378 & 0.545 & 0.373 & 0.534 & 0.383 & 0.550\\
	& Brightness & 0.341 & 0.503 & 0.391 & 0.557 & 0.348 & 0.510 & 0.417 & 0.583\\
	& Contrast & 0.297 & 0.454 & 0.286 & 0.442 & 0.270 & 0.422 & 0.206 & 0.338\\
	& Saturation & 0.310 & 0.470 & 0.284 & 0.396 & 0.245 & 0.390 & 0.211 & 0.346\\
	\hline
	\multirow{5}{*}{\textbf{Global model}} & - & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& Embed & 0.400 & 0.566 & 0.438 & 0.607 & 0.333 & 0.497 & 0.454 & 0.622\\
	& Brightness & 0.500 & 0.663 & 0.629 & 0.770 & 0.452 & 0.620 & 0.625 & 0.767\\
	& Contrast & 0.404 & 0.573 & 0.539 & 0.699 & 0.334 & 0.499 & 0.562 & 0.718\\
	& Saturation & 0.420 & 0.586 & 0.475 & 0.641 & 0.346 & 0.511 & 0.441 & 0.609\\
	\hline
	\multirow{5}{*}{\textbf{Global-Local model}} & - & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& Embed & 0.372 & 0.541 & 0.451 & 0.619 & 0.393 & 0.559 & 0.375 & 0.542\\
	& Brightness & 0.495 & 0.659 & 0.613 & 0.759 & 0.465 & 0.632 & 0.604 & 0.751\\
	& Contrast & 0.408 & 0.577& 0.545 & 0.704 & 0.414 & 0.583 & 0.503 & 0.666\\
	& Saturation & 0.402 & 0.570 & 0.491 & 0.657 & 0.387 & 0.555 & 0.490 & 0.654\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for different augmentations on the test images.}
	\label{table:augmentation-comparison}
\end{table}

\subsubsection{Performance on an unseen data set}

In a second experiment, the performance is evaluated on an additional, unseen data set. For this purpose, the data set of the Diabetes Foot Ulcer Segmentation Challenge 2021 \cite{Wang2020} is used. More details can be found in section \ref{sec:data-sets}. The non-augmented images are loaded, resized to 192x192\,px and then tested with the already trained models. The performance metrics are displayed in table \ref{table:dataset-comparison}, in comparison with the performance on test data of the original WSNet data set.


\begin{table}[htb!]
	\centering
	\begin{tabular}{l | c ||c | c || c | c || c | c || c | c||}
	& & \multicolumn{2}{|c||}{U-Net} & \multicolumn{2}{|c||}{LinkNet} & \multicolumn{2}{|c||}{PSPNet} & \multicolumn{2}{|c||}{FPN} \\
	\hline
	& Data set & IoU & Dice & IoU & Dice & IoU & Dice & IoU & Dice \\
	\hline\hline
	\multirow{2}{*}{\textbf{Local model}} & WSNet & 0.359 & 0.523 & 0.398 & 0.564 & 0.373 & 0.538 & 0.408 & 0.574 \\
	& DFUC & 0.262 & 0.411 & 0.203 & 0.335 & 0.231 & 0.372 & 0.231 & 0.372\\
	\hline
	\multirow{2}{*}{\textbf{Global model}} & WSNet & 0.504 & 0.668 & 0.631 & 0.772 & 0.458 & 0.627 & 0.632 & 0.772 \\
	& DFUC & 0.214 & 0.350 & 0.276 & 0.428 & 0.155 & 0.265 & 0.238 & 0.380\\
	\hline
	\multirow{2}{*}{\textbf{Global-Local model}} & WSNet & 0.495 & 0.658 & 0.618 & 0.763 & 0.476 & 0.642 & 0.612 & 0.758\\
	& DFUC & 0.201 & 0.330 & 0.181 & 0.304 & 0.181 & 0.304 & 0.215 & 0.349\\
	\end{tabular}
	\caption{IoU-Scores and Dice Coefficients for the four different models with each Global-Local, Global and Local architecture compared for the WSNet and the Diabetes Foot Ulcer Segmentation Challenge (DFUC) 2021 data.}
	\label{table:dataset-comparison}
\end{table}

\begin{itemize}
	\item worsens performance by huge amount for every model
	\item probably training on mix needed but no time in the scope of the project
	\item means results are not directly transferable to new models
\end{itemize}

TODO
